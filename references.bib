
@incollection{saito1988,
  author={Saito, Kazumi and Nakano, Ryohei},
  abstract = {The applicability of PDP (parallel distributed processing) models to knowledge processing is clarified. The authors evaluate the diagnostic capabilities of a prototype medical diagnostic expert system based on a multilayer network. After having been trained on only 300 patients, the prototype system shows diagnostic capabilities almost equivalent to those of a symbolic expert system. Symbolic knowledge is extracted from what the multilayer network has learned. The extracted knowledge is compared with doctors' knowledge. Moreover, a method 
to extract rules from the network and usage of the rules in a confirmation process are proposed.},
  booktitle={IEEE 1988 International Conference on Neural Networks}, 
  title={Medical diagnostic expert system based on PDP model}, 
  year={1988},
  volume={1},
  pages={255-262},
  doi={10.1109/ICNN.1988.23855}}

@article{gallant1988,
author = {Gallant, Stephen I.},
title = {Connectionist Expert Systems},
year = {1988},
issue_date = {Feb. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/42372.42377},
doi = {10.1145/42372.42377},
abstract = {Connectionist networks can be used as expert system knowledge bases. Furthermore, such networks can be constructed from training examples by machine learning techniques. This gives a way to automate the generation of expert systems for classification problems.},
journal = {Commun. ACM},
month = {feb},
pages = {152–169},
numpages = {18}
}

@incollection{saito1990,
  title={Automatic Extraction of Classification Rules},
  author={Saito, Kazumi and Nakano, Ryohei},
  booktitle={International Neural Network Conference, July 9-13, 1990, Palais des Congres, Paris, France},
  pages={379--382},
  year={1990}
}

@incollection{baba1990,
  author={Kenji Baba and Ichiro Enbutu and Mikio Yoda},
  booktitle={1990 IJCNN International Joint Conference on Neural Networks}, 
  title={Explicit representation of knowledge acquired from plant historical data using neural network}, 
  abstract = {A causal index, which translates implicit knowledge contained in a neural network into an explicit representation, is proposed. A backpropagation-based learning algorithm which suppresses the nondominant causal relationships to improve association accuracy is developed using the index. The validity of the proposed algorithm is investigated using historical data from a coagulant injection operation in a water-purification plant. Learning of relationships between the operational factor (coagulant injection rate) and influence factors (water qualities and floc image properties) in plant operations is carried out 
by the proposed algorithm. Improvements in the association ability for unknown conditions and in the 
reliability of acquired knowledge are realized.},
  year={1990},
  volume={3},
  pages={155-160},
  doi={10.1109/IJCNN.1990.137838}}


@incollection{sestito1990,
  author={Sabrina Sestito and Tharam S. Dillon},
  booktitle={[1990] Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence}, 
  title={Machine learning using single-layered and multi-layered neural networks}, 
  abstract = {Methods are proposed which automatically extract a high level knowledge representation in the form of rules from the lower level representation used by neural networks. The strength of neural networks in dealing with noise has made it possible to produce correct rules in a noisy domain. Results obtained when applying the proposed method to a noisy domain suggest that this method can be used in real-world domains. 
It is believed that this work will lead to an area of machine learning which uses neural networks as the basis of knowledge acquisition which can deal with real-world difficulties.},
  year={1990},
  pages={269-275},
  doi={10.1109/TAI.1990.130346}}

@incollection{mcmillan1991a,
  title={The connectionist scientist game: rule extraction and refinement in a neural network},
  abstract = {Scientific induction involves an iterative process of
hypothesis formulation, testing, and refinement. People
in ordinary life appear to undertake a similar process in
explaining their world. We believe that it is instructive
to study rule induction in connectionist systems from a
similar perspective. We propose an approach, called the
Connectionist Scientist Game, in which symbolic condition-action rules are extracted from the learned connection strengths in a network, thereby forming explicit
hypotheses about a domain. The hypotheses are tested
by injecting the rules back into the network and continuing the training process. This extraction-injection process continues until the resulting rule base adequately
characterizes the domain. By exploiting constraints
inherent in the domain of symbolic string-to-string mappings, we show how a connectionist architecture called
RuleNet can induce explicit, symbolic condition-action
rules from examples. RuleNet’s performance is far superior to that of a variety of alternative architectures we’ve
examined. RuleNet is capable of handling domains having both symbolic and subsymbolic components, and
thus shows greater potential than purely symbolic learning algorithms. The formal string manipulation task performed by RuleNet can be viewed as an abstraction of
several interesting cognitive models in the connectionist
literature, including case role assignment and the mapping of orthography to phonology.},  
  author={McMillan, Clayton and Mozer, Michael C and Smolensky, Paul},
  booktitle={Proceedings of the 13th Annual Conference of the Cognitive Science Society},
  pages={424--430},
  year={1991}
}

@inproceedings{mcmillan1991b,
 author = {McMillan, Clayton and Mozer, Michael C and Smolensky, Paul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 pages = {969--976},
abstract = {We describe a neural network, called RuleNet, that learns explicit, symbolic condition-action rules in a formal 
string manipulation domain. RuleNet discovers functional categories over elements of the domain, and, at various points during 
learning, extracts rules that operate on these categories. The rules are then injected back into RuleNet and training continues, 
in a process called iterative projection. By incorporating rules in this way, RuleNet exhibits 
enhanced learning and generalization performance over alternative neural net approaches. By integrating symbolic 
rule learning and subsymbolic category learning, RuleNet has capabilities that go beyond a purely symbolic system. 
We show how this architecture can be applied to the problem 
of case-role assignment in natural language processing, yielding a novel rule-based solution.},
 publisher = {Morgan-Kaufmann},
 title = {Rule Induction through Integrated Symbolic and Subsymbolic Processing},
 url = {https://proceedings.neurips.cc/paper/1991/file/cf67355a3333e6e143439161adc2d82e-Paper.pdf},
 volume = {4},
 year = {1991}
}

@article{sestito1991,
  title={Using single-layered neural networks for the extraction of conjunctive rules and hierarchical classifications},
  author={Sestito, Sabrina and Dillon, Tharam},
  journal={Applied Intelligence},
  abstract = {Machine Learning is an area concerned with the automation of the process of knowledge acquisition. 
Neural networks generally represent their knowledge at the lower level, while knowledge based systems use higher 
level knowledge representations. The method we propose here, provides a technique which automatically allows us 
to extract production rules from the lower level representation used by a single-layered neural networks 
trained by Hebb's rule. Even though a single-layered neural network can not model complex, nonlinear domains, 
their strength in dealing with noise has enabled us to produce correct rules in a noisy domain.},
  volume={1},
  pages={157--173},
  year={1991},
  doi = {10.1007/BF00058881},
  publisher={Springer},
  keywords = {type: neural_networks, machine_learning, knowledge_acquisition}
}

@inproceedings{fu1991,
  title={Rule Learning by Searching on Adapted Nets},
  author={Limin Fu},
  booktitle={9th National Conference on Artificial Intelligence, July 14-19, 1991, Anaheim, California},
abstract = {If the back propagation network can produce an inference structure with high and robust performance, 
then it is sensible to extract rules from it. 
The KT algonthm is a novel algonthm for generating rules from an adapted net efficiently. 
The algorithm is able to deal with both single-layer and multi-layer networks, and can learn 
both confirming and disconfirming rules. Empirically, the algorithm is 
demonstrated in the domain of wind shear detection by infrared sensors with success.},
  year={1991},
  publisher = {AAAI Press},
  address = {Menlo Park, California},
  pages={590--595}
}

@article{towell1993,
  title={Extracting refined rules from knowledge-based neural networks},
  author={Towell, Geoffrey G and Shavlik, Jude W},
  journal={Machine Learning},
  abstract ={Neural networks, despite their empirically proven abilities, 
have been little used for the refinement of existing knowledge because this 
task requires a three-step process. First, knowledge must be inserted into 
a neural network. Second, the network must be refined. Third, the refined 
knowledge must be extracted from the network. We have previously described a 
method for the first step of this process. Standard neural learning techniques 
can accomplish the second step. In this article, we propose and empirically evaluate 
a method for the final, and possibly most difficult, step. Our method efficiently extracts 
symbolic rules from trained neural networks. The four major results of empirical tests 
of this method are that the extracted rules 1) closely reproduce the accuracy of the 
network from which they are extracted; 2) are superior to the rules produced by methods 
that directly refine symbolic rules; 3) are superior to those produced by previous techniques for 
extracting rules from trained neural networks; and 4) are “human comprehensible.” Thus, this method 
demonstrates that neural networks can be used to effectively refine symbolic knowledge. Moreover, the 
rule-extraction technique developed herein contributes to the understanding of how symbolic and 
connectionist approaches to artificial intelligence can be profitably integrated.},
  volume={13},
  pages={71--101},
  year={1993},
  publisher={Springer},
  doi = {10.1007/BF00993103},
  keywords={type:theory_refinement, integrated_learning, representational_shift, rule_extraction_from_neural_networks}
}

@inproceedings{craven1993b,
  title={Understanding neural networks via rule extraction and pruning},
  author={Craven, Mark W and Shavlik, Jude W},
  booktitle={Proceedings of the 1993 Connectionist Models Summer School},
  pages={184},
  year={1993},
  organization={Psychology Press},
  %abstract={},
  url={https://books.google.co.ma/books?hl=en&lr=&id=0SEBAwAAQBAJ&oi=fnd&pg=PA184&dq=Understanding+neural+networks+via+rule+extraction+and+pruning&ots=U5vxpNw3CH&sig=1eNzxACVFHmoEBk4M3fldMCKNZY&redir_esc=y#v=onepage&q=Understanding%20neural%20networks%20via%20rule%20extraction%20and%20pruning&f=false},
  %keywords={}
}

@techreport{thrun1993,
  title={Extracting provably correct rules from artificial neural networks},
  author={Sebastian Thrun},
  series={Technical report iai-tr-93-5},
  publisher={University of Bonn, Institut fur Informatik III},
  abstract={Although connectionist learning procedures have been applied successfully to
a variety of real-world scenarios, artificial neural networks have often been
criticized for exhibiting a low degree of comprehensibility. Mechanisms that
automatically compile neural networks into symbolic rules offer a promising
perspective to overcome this practical shortcoming of neural network representations.
This paper describes an approach to neural network rule extraction based on Validity Interval Analysis(VI-Analysis). 
VI-Analysis is a generic tool for extracting
symbolic knowledge from Backpropagation-style artificial neural networks. It
does this by propagating whole intervals of activations through the network in
both the forward and backward directions. In the context of rule extraction, these
intervals are used to prove or disprove the correctness of conjectured rules. We
describe techniques for generating and testing rule hypotheses, and demonstrate
these using some simple classification tasks including the MONK’s benchmark
problems. Rules extracted by VI-Analysis are provably correct. No assumptions
are made about the topology of the network at hand, as well as the procedure
employed for training the network.},
  url={https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.2110&rep=rep1&type=pdf},
  keywords={type:machine_learning, artificial_neural_networks, rule_extraction, validity_interval
analysis, symbolic_and_subsymbolic_representations},
  year={1993},
  publisher={Citeseer}
}

@incollection{craven1993a,
title = {Learning Symbolic Rules Using Artificial Neural Networks},
booktitle = {Machine Learning Proceedings 1993},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {73-80},
year = {1993},
isbn = {978-1-55860-307-3},
doi = {10.1016/B978-1-55860-307-3.50016-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603073500162},
author = {Mark W. Craven and Jude W. Shavlik},
abstract = {A distinct advantage of symbolic learning algorithms over artificial 
neural networks is that typically the concept representations they form are more easily understood 
by humans. One approach to understanding the representations formed by neural networks is to extract 
symbolic rules from trained networks. In this paper we describe and investigate an approach for extracting rules 
from networks that uses (1) the NOFM extraction algorithm, and (2) the network training method of soft weight-sharing. 
Previously, the NOFM algorithm had been successfully applied only to knowledge-based neural networks. 
Our experiments demonstrate that our extracted rules generalize better than rules learned using the C4.5 system. 
In addition to being accurate, our extracted rules are also reasonably comprehensible.},
%keywords={}
}

@incollection{goh1993,
  author={Tiong-Hwee Goh},
  booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)}, 
  title={Semantic extraction using neural network modelling and sensitivity analysis}, 
  year={1993},
  volume={1},
  pages={1031-1034},
  doi={10.1109/IJCNN.1993.714088},
  abstract={We present three methods of using neural network modelling and sensitivity analysis to extract semantics 
for given historical data of a given system. First, neural network modelling and sensitivity analysis 
is used to determine the decision boundaries of the system. Several test cases including a 
simple credit rating system are described to illustrate the use and effectiveness of this method. 
Secondly, it is used for causal inferencing of the system. That is, determining which inputs has 
the largest effect on the output of the system. In addition, causal inference under 
different input conditions was tested. This is done by applying the index on a subset of the input data. 
Typical problems like the decoder and parity are tested and discussed. Lastly it is used to analyze historical data for 
detection of exceptions or special input cases. In all three methods, the system being studied which is available only in 
the form of input-output pair data, is first modelled using a neural network. Sensitivity analysis is then applied to the trained 
network to extract semantics learned by the network.},
  keywords={type: neural_nets, knowledge_acquisition, financial_data_processing, sensitivity_analysis, computational_linguistics, 
semantic_networks}
}

@article{sestito1993,
author = {Sestito, Sabrina and Dillon, Tharam},
title = {Knowledge acquisition of conjunctive rules using multilayered neural networks},
journal = {International Journal of Intelligent Systems},
volume = {8},
number = {7},
pages = {779-805},
doi = {https://doi.org/10.1002/int.4550080704},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.4550080704},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.4550080704},
abstract = {Abstract A major bottleneck in developing knowledge-based systems is the acquisition of knowledge. 
Machine learning is an area concerned with the automation of this process of knowledge acquisition. 
Neural networks generally represent their knowledge at the lower level, while knowledge-based systems use higher-level knowledge 
representations. the method we propose here provides a technique that automatically allows us to extract conjunctive rules 
from the lower-level representation used by neural networks, the strength of neural networks in dealing with noise has 
enabled us to produce correct rules in a noisy domain. Thus we propose a method that uses neural networks as the basis for 
the automation of knowledge acquisition and can be applied to noisy, realworld domains. © 1993 John Wiley \& Sons, Inc.},
year = {1993},
%keywords={}
}

@inproceedings{thrun1994,
 author = {Thrun, Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 %pages = {},
 publisher = {MIT Press},
 title = {Extracting Rules from Artificial Neural Networks with Distributed Representations},
 abstract={Although artificial neural networks have been applied in a variety of real-world scenarios
with remarkable success, they have often been criticized for exhibiting a low degree of
human comprehensibility. Techniques that compile compact sets of symbolic rules out
of artificial neural networks offer a promising perspective to overcome this obvious
deficiency of neural network representations.
This paper presents an approach to the extraction of if-then rules from artificial neural
networks. Its key mechanism is validity interval analysis, which is a generic
tool for extracting symbolic knowledge by propagating rule-like knowledge through
Backpropagation-style neural networks. Empirical studies in a robot arm domain illustrate
the appropriateness of the proposed method for extracting rules from networks with
real-valued and distributed representations.},
 url = {https://proceedings.neurips.cc/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf},
 volume = {7},
 year = {1994},
 %keywords={}
}

@incollection{craven1994,
title = {Using Sampling and Queries to Extract Rules from Trained Neural Networks},
editor = {William W. Cohen and Haym Hirsh},
booktitle = {Machine Learning Proceedings 1994},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {37-45},
year = {1994},
isbn = {978-1-55860-335-6},
doi = {https://doi.org/10.1016/B978-1-55860-335-6.50013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500131},
author = {Mark W. Craven and Jude W. Shavlik},
abstract = {Concepts learned by neural networks are difficult to understand because they are represented using 
large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract 
symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that 
operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a 
learning problem. In addition to learning from training examples, our method exploits the property that networks can be efficiently queried. 
We describe algorithms for extracting both conjunctive and M-of-N rules, and present experiments that show that our method is more 
efficient than conventional search-based approaches.},
%keywords={}
}


@incollection{sethi1994,
title = {Symbolic approximation of feedforward neural networks},
editor = {Edzard S. GELSEMA and Laveen S. KANAL},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {16},
pages = {313-324},
year = {1994},
booktitle = {Pattern Recognition in Practice IV},
issn = {0923-0459},
doi = {https://doi.org/10.1016/B978-0-444-81892-8.50032-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444818928500328},
author = {Ishwar K. Sethi and Jae H. Yoo},
abstract = {Multiple layer feedforward neural networks are often viewed as black boxes as 
the knowledge stored in the connection weights of these networks is generally considered 
incomprehensible. This paper suggests a solution to this deficiency of neural networks by 
proposing a backtracking tree search procedure for converting the weights of a neuron into a 
symbolic representation and demonstrating its use for understanding and symbolic approximation of 
feedforward neural networks. Several examples are presented to illustrate the proposed symbolic mapping of neurons.},
%keywords={}
}

@inproceedings{blasig1993,
 author = {Blasig, Reinhard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 pages = {1993--1100},
 publisher = {Morgan-Kaufmann},
 title = {GDS: Gradient Descent Generation of Symbolic Classification Rules},
 url = {https://proceedings.neurips.cc/paper/1993/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf},
 volume = {6},
 year = {1993},
 abstract = {Imagine you have designed a neural network that successfully learns a 
complex classification task. What are the relevant input features 
the classifier relies on and how are these features combined to produce the 
classification decisions? There are applications where a deeper insight into the structure of 
an adaptive system and thus into the underlying classification problem may well be as important 
as the system's performance characteristics, e.g. in economics or medicine. GDS is a backpropagation-based 
training scheme that produces networks transformable into an equivalent and concise set of IF-THEN rules. 
This is achieved by imposing penalty terms on the network parameters that adapt the network to the expressive 
power of this class of rules. Thus during training we simultaneously minimize classification and transformation error. 
Some real-world tasks demonstrate the viability of our approach.},
%keywords={}
}

@inproceedings{alexander1994,
 author = {Alexander, Jay and Mozer, Michael C},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 pages = {609--616},
 publisher = {MIT Press},
 title = {Template-Based Algorithms for Connectionist Rule Extraction},
 url = {https://proceedings.neurips.cc/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf},
 volume = {7},
 year = {1994},
 abstract={Casting neural network weights in symbolic terms is crucial for interpreting and 
explaining the behavior of a network. Additionally, in some domains, a symbolic description 
may lead to more robust generalization. We present a principled approach to symbolic rule extraction 
based on the notion of weight templates, parameterized regions of weight space corresponding to specific 
symbolic expressions. With an appropriate choice of representation, we show how template parameters may 
be efficiently identified and instantiated to yield the optimal match to a unit's actual weights. 
Depending on the requirements of the application domain, our method can accommodate arbitrary disjunctions 
and conjunctions with O(k) complexity, simple n-of-m expressions with O( k!) complexity, or a more general 
class of recursive n-of-m expressions with O(k!) complexity, where k is the number of inputs to a unit. 
Our method of rule extraction offers several benefits over alternative approaches in the literature, and 
simulation results on a variety of problems demonstrate its effectiveness.},
%keywords={}
}

@article{tickle1994,
  title={DEDEC: Decision Detection by Rule Extraction from Neural Networks. Queensland University of Technology},
  author={Tickle, AB and Orlowski, M and Diederich, J},
  journal={Neurocomputing Research Center QUT NRC},
  year={1994},
  %abstract={},
  %url={},
  %keywords={}
}

@article{fu1994,
  author={LiMin Fu},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Rule generation from neural networks}, 
  year={1994},
  volume={24},
  number={8},
  pages={1114-1124},
  doi={10.1109/21.299696},
  keywords={type: neural_network, knowledge-based_system, machine_learning, rule-based_system}
}

@incollection{lu1995,
author = {Hongjun Lu and Rudy Setiono and Huan Liu},
title = {NeuroRule: {A} Connectionist Approach to Data Mining},
booktitle = {Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995},
year = {1995},
abstract={Classification, which involves finding rules that partition a given data set into disjoint groups, 
is one class of data mining problems. Approaches proposed so far for mining classification rules for large 
databases are mainly decision tree based symbolic learning methods. The connectionist approach based on neural 
networks has been thought not well suited for data mining. One of the major reasons cited is that knowledge 
generated by neural networks is not explicitly represented in the form of rules suitable for verification or 
interpretation by humans. This paper examines this issue With our newly developed algorithms, rules which are similar to, 
or more concise than those generated by the symbolic methods can be extracted from the neural networks. 
The data mining process using neural networks with the emphasis on rule extraction is described. Experimenal results 
and comparison with previously published works are presented.},
url={https://arxiv.org/abs/1701.01358},
%keywords={}
}

@inproceedings{setiono1995,
  title={Understanding neural networks via rule extraction},
  author={Setiono, Rudy and Liu, Huan},
  booktitle={Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI), Montreal, Quebec, Canada August 20-25,1995},
  volume={1},
  pages={480--485},
  year={1995},
  url={https://www.ijcai.org/Proceedings/95-1/Papers/063.pdf},
  abstract ={Although backpropagation neural networks generally predict better 
than decision trees do for pattern classification problems, 
they are often regarded as black boxes, i.e., their predictions are not as interpretable 
as those of decision trees. This paper argues that this is because there has been no proper 
technique that enables us to do so. With an algorithm that can extract rules, by drawing 
parallels with those of decision trees, we show that the predictions of a network can 
be explained via rules extracted from it, thereby, the network can be understood. 
Experiments demonstrate that rules extracted from neural networks are comparable 
with those of decision trees in terms of predictive accuracy, number of rules 
and average number of conditions for a rule; they preserve high predictive accuracy of original networks.},
  %keywords={}
}

@inproceedings{craven1995,
 author = {Craven, Mark and Shavlik, Jude},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 pages = {24--30},
 publisher = {MIT Press},
 title = {Extracting Tree-Structured Representations of Trained Networks},
 url = {https://proceedings.neurips.cc/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf},
 volume = {8},
 year = {1995},
 abstract={A significant limitation of neural networks is that the representations they 
learn are usually incomprehensible to humans. We present a novel algorithm, TREPAN, 
for extracting comprehensible, symbolic representations from trained neural networks. 
Our algorithm uses queries to induce a decision tree that approximates the concept 
represented by a given network. Our experiments demonstrate that TREPAN is able to 
produce decision trees that maintain a high level of fidelity to their respective networks while 
being comprehensible and accurate. Unlike previous work in this area, our algorithm is general in its 
applicability and scales well to large networks and problems with high-dimensional input spaces.},
% keywords={}
}

@ARTICLE{setiono1996,  
author={Setiono, R. and Huan Liu},  
journal={Computer},   
title={Symbolic representation of neural networks},   
year={1996},  
volume={29},  
number={3},  
pages={71-77},  
doi={10.1109/2.485895},
abstract={Neural networks often surpass decision trees in predicting pattern 
classifications, but their predictions cannot be explained. This algorithm's 
symbolic representations make each prediction explicit and understandable. 
Our approach to understanding a neural network uses symbolic rules to 
represent the network decision process. The algorithm, NeuroRule, extracts 
these rules from a neural network. The network can be interpreted by the rules which, 
in general, preserve network accuracy and explain the prediction process. We based 
NeuroRule on a standard three layer feed forward network. NeuroRule consists of 
four phases. First, it builds a weight decay backpropagation network so that 
weights reflect the importance of the network's connections. Second, it prunes 
the network to remove irrelevant connections and units while maintaining the 
network's predictive accuracy. Third, it discretizes the hidden unit activation 
values by clustering. Finally, it extracts rules from the network with discretized hidden unit activation values.},
%keywords={}
}

@ARTICLE{lu1996,  
author={Hongjun Lu and Setiono, R. and Huan Liu},  
journal={IEEE Transactions on Knowledge and Data Engineering},   
title={Effective data mining using neural networks},   
year={1996},  
volume={8},  
number={6},  
pages={957-961},  
doi={10.1109/69.553163},
abstract={Classification is one of the data mining problems receiving great attention recently in 
the database community. The paper presents an approach to discover symbolic classification rules 
using neural networks. Neural networks have not been thought suited for data mining because how 
the classifications were made is not explicitly stated as symbolic rules that are suitable for 
verification or interpretation by humans. With the proposed approach, concise symbolic rules 
with high accuracy can be extracted from a neural network. The network is first trained to 
achieve the required accuracy rate. Redundant connections of the network are then removed 
by a network pruning algorithm. The activation values of the hidden units in the network 
are analyzed, and classification rules are generated using the result of this analysis. 
The effectiveness of the proposed approach is clearly demonstrated by the experimental 
results on a set of standard data mining test problems.},
keywords={type: data_mining, neural_networks, rule_extraction, network_pruning, classification}
}

@inproceedings{hayward1996,
author={Hayward, Ross and Tickle, Alan and Diederich, Joachim},
editor={Wermter, Stefan and Riloff, Ellen and Scheler, Gabriele},
title={Extracting rules for grammar recognition from Cascade-2 networks},
booktitle={Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing},
year={1996},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={48--60},
abstract={Any symbolic representation of how a neural network decides on a particular 
classification is important, not only for user acceptance but also for rule refinement and 
network learning. This paper describes a decompositional rule extraction technique which 
generates rules governing the firing of individual nodes in a feedforward neural network. 
The technique employs heuristics to reduce the complexity in searching for rules which 
explain the behaviour of a neural network trained with sparsely coded data. The hidden 
and output units within the constructed networks are considered to represent distinct 
rules which govern the classification of patterns. A symbolic representation of the 
complete network can be gained from aggregation of these rules. Since the algorithm 
is not constrained to a particular network architecture, we focus on a comparison of 
the rule sets gained from networks constructed using the Cascade 2 algorithm and 
multi-layer perceptrons trained using backpropagation. The specific networks of interest 
are those trained to recognise if simple fixed length sentences are grammatically correct.},
isbn={978-3-540-49738-7},
%keywords={}
}

@INPROCEEDINGS{bloomer1996,  
author={Bloomer, W.F. and Dillon, T.S. and Witten, M.},  
booktitle={1996 IEEE International Conference on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929)},   
title={Hybrid BRAINNE: a method for developing symbolic disjunctive rules from a hybrid neural network},   
year={1996},  
volume={4},    
pages={2745-2750},  
doi={10.1109/ICSMC.1996.561374},
abstract={A method for learning disjunctive rules using a combination of two existing 
neural network schemes is proposed. The hybrid network consists of two layers; 
the first is an unsupervised network while the second is a supervised network. 
The first layer is used for ordering the inputs of training instances into clusters. 
Initial rules are extracted from this layer using an existing technique called Unsupervised 
BRAINNE. These rules are then fed into the second layer which is trained using the delta rule. 
The second layer is then examined to determine which clusters define the output nodes. 
This method is able to identify disjunctive rules directly rather than utilising a 
generate and test paradigm as was used in previous supervised versions of BRAINNE.},
keywords={}
}

@inproceedings{decloedt1996,
  title={RULE\_OUT method: A new approach for knowledge explicitation from trained ann},
  author={Decloedt, L{\"o}ic and Os{\'o}rio, Fernando and Amy, Bernard},
  booktitle={Proceedings of the AISB’96 - Workshop on Rule Extraction from Trained Neural Nets},
  pages={34--42},
  year={1996},
  editor={Andrews and Diederich},
  abstract={Artificial Neural Networks have been applied in many different domains with success. 
Their main advantage come from their generalisation ability on a learned base of examples. 
On the other hand, their major drawback comes from their lack of justification abilities. 
So, several methods have been designed to explicit their knowledge in a symbolic form, 
in order to get it comprehensible for a human operator. We present here an ANN knowledge 
explicitation system, included in a more general neuro-symbolic hybrid AI system: 
the INSS system. Usually, existing explicitation systems attempt to express the whole 
part of the ANN knowledge. Here, choice has been made to represent only its more significant 
part. In order to do this, we have developed methods based on the incrementality of the 
applied ANN learning method, and on network simplification. We present these methods in 
this paper; then, some experimental results obtained on the Monk's problems are shown.},
  url={https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.8091&rep=rep1&type=pdf},
  %keywords={}
}

@article{setiono1997b,
    author = {Setiono, Rudy},
    title = "{Extracting Rules from Neural Networks by Pruning and Hidden-Unit Splitting}",
    journal = {Neural Computation},
    volume = {9},
    number = {1},
    pages = {205-225},
    year = {1997},
    month = {01},
    abstract = {An algorithm for extracting rules from a standard three-layer 
feedforward neural network is proposed. The trained network is first pruned 
not only to remove redundant connections in the network but, more important, 
to detect the relevant inputs. The algorithm generates rules from the pruned network by 
considering only a small number of activation values at the hidden units. If the number 
of inputs connected to a hidden unit is sufficiently small, then rules that describe how 
each of its activation values is obtained can be readily generated. Otherwise the hidden 
unit will be split and treated as output units, with each output unit corresponding to an activation value. 
A hidden layer is inserted and a new subnetwork is formed, trained, and pruned. This process is repeated 
until every hidden unit in the network has a relatively small number of input units connected to it. 
Examples on how the proposed algorithm works are shown using real-world data arising from molecular 
biology and signal processing. Our results show that for these complex problems, the algorithm can 
extract reasonably compact rule sets that have high predictive accuracy rates.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.1.205},
    url = {https://doi.org/10.1162/neco.1997.9.1.205},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/1/205/813397/neco.1997.9.1.205.pdf},
    %keywords={}
}

@article{setiono1997a,
title = {NeuroLinear: From neural networks to oblique decision rules},
journal = {Neurocomputing},
volume = {17},
number = {1},
pages = {1-24},
year = {1997},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(97)00038-6},
url = {https://www.sciencedirect.com/science/article/pii/S0925231297000386},
author = {Rudy Setiono and Huan Liu},
keywords = {Rule extraction, Oblique-rule, Pruning, Discretization},
abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks 
that have been trained for classification of patterns. Each condition of an oblique decision 
rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily 
axisparallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a 
significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. 
We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. 
Our experimental results on real-world datasets show that the system is effective in extracting compact and 
comprehensible rules with high predictive accuracy from neural networks.},
keywords={type: rule_extraction, oblique-rule, pruning, discretization}
}

@article{arbatli1997,
title = {Rule extraction from trained neural networks using genetic algorithms},
journal = {Nonlinear Analysis: Theory, Methods & Applications},
volume = {30},
number = {3},
pages = {1639-1648},
year = {1997},
issn = {0362-546X},
doi = {https://doi.org/10.1016/S0362-546X(96)00267-2},
url = {https://www.sciencedirect.com/science/article/pii/S0362546X96002672},
author = {A. {Duygu Arbatli} and H. {Levent Akin}},
keywords = {type: rule_extraction, rule_improvement, artificial_neural_networks, genetic_algorithms},
%abstract={}
}

@article{hruschka1970,
  title={Rule Extraction from Neural Networks in Data Mining Applications},
  author={Hruschka, Eduardo R},
  journal={WIT Transactions on Information and Communication Technologies},
  volume={22},
  year={1970},
  publisher={WIT Press},
  url={https://www.witpress.com/elibrary/wit-transactions-on-information-and-communication-technologies/22/6942},
  abstract={This work deals with the efficient discovery of valuable and nonobvious
information from large collections of data, using Computacional Intelligence
tools. For this purpose, a . study about knowledge acquirement from supervised
neural networks employed for classification problems is presented. An
algorithm for rule extraction from neural networks, based on the work by Lu et
al. [1] in 1996, is developed. This algorithm, named Modified RX, is
experimentally evaluated in three different domains. The results are compared to
those obtained by classification trees. In respect of the efficacy , one observes
that the successful application of the algorithm mainly depends on the
knowledge representation acquired by the conecctionist model, while the
eflcciency only depends on the neural network training time.},
%keywords={}
}

@article{duch1998,
  title={Extraction of logical rules from neural networks},
  author={Duch, W{\l}odzis{\l}aw and Adamczak, Rafa{\l} and Gr{\k{a}}bczewski, Krzysztof},
  journal={Neural Processing Letters},
  volume={7},
  pages={211--219},
  year={1998},
  publisher={Springer},
  doi={10.1023/A:1009670302979},
  abstract={Three neural-based methods for extraction of logical rules from data are presented. 
These methods facilitate conversion of graded response neural networks into networks performing 
logical functions. MLP2LN method tries to convert a standard MLP into a network performing logical operations 
(LN). C-MLP2LN is a constructive algorithm creating such MLP networks. 
Logical interpretation is assured by adding constraints to the cost function, 
forcing the weights to ±1 or 0. Skeletal networks emerge ensuring that a minimal number of 
logical rules are found. In both methods rules covering many training examples are generated 
before more specific rules covering exceptions. The third method, FSM2LN, is based on the 
probability density estimation. Several examples of performance of these methods are presented.},
  keywords={type: backpropagation, feature_selection, logical_rule_extraction, MLP, neural_networks, probability_density_estimation}
}

@INPROCEEDINGS{bologna1998,  
author={Bologna, G. and Pellegrini, C.},  
booktitle={1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence 
(Cat. No.98CH36227)},   
title={Constraining the MLP power of expression to facilitate symbolic rule extraction},   
year={1998},  
volume={1},  
number={},  
pages={146-151},  
doi={10.1109/IJCNN.1998.682252},
abstract={Extracting symbolic rules from multilayer perceptrons is an important open question, 
especially when input neurons are continuous. To solve this problem we constrain the power of expression of a 
standard MLP with threshold functions in the hidden layer. In this case, hyper-plane equations are 
precisely determined and translated into symbolic rules. We illustrate our interpretable MLP (IMLP) 
in two applications; one from iris classification, and one from coronary heart disease diagnosis. 
In spite of the reduced power of expression, IMLP is able to give close mean predictive accuracy with respect to a standard MLP.},
%keywords={}
}

@ARTICLE{schmitz1999,
  author={Schmitz, Gregor P.J. and Aldrich, Chris and Gouws, Francois S.},
  journal={IEEE Transactions on Neural Networks}, 
  title={ANN-DT: an algorithm for extraction of decision trees from artificial neural networks}, 
  year={1999},
  volume={10},
  number={6},
  pages={1392-1401},
  doi={10.1109/72.809084},
  abstract={Although artificial neural networks can represent a variety of complex systems with a high degree of accuracy, these connectionist models are difficult to interpret. This significantly limits the applicability of neural networks in practice, especially where a premium is placed on the comprehensibility or reliability of systems. A novel artificial neural-network decision tree algorithm (ANN-DT) is therefore proposed, which extracts binary decision trees from a trained neural network. The ANN-DT algorithm uses the neural network to generate outputs for samples interpolated from the training data set. In contrast to existing techniques, ANN-DT can extract rules from feedforward neural networks with continuous outputs. These rules are extracted from the neural network without making assumptions about the internal structure of the neural network or the features of the data. A novel attribute selection criterion based on a significance analysis of the variables on the neural-network output is examined. It is shown to have significant benefits in certain cases when compared with the standard criteria of minimum weighted variance over the branches. In three case studies the ANN-DT algorithm compared favorably with CART, a standard decision tree algorithm.},
  keywords={type: decision_trees, hybrid_systems, induction, neural_networks, rule_extraction, 
sensitivity_analysis}
}

@ARTICLE{taha1999,
  author={Taha, I.A. and Ghosh, J.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Symbolic interpretation of artificial neural networks}, 
  year={1999},
  volume={11},
  number={3},
  pages={448-463},
  doi={10.1109/69.774103},
  abstract={Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
  keywords={type: rule_extraction, hybrid_systems, knowledge_refinement, neural_networks, rule_evaluation}
}

@article{gupta1999,  
author={Gupta, Amit and Sang Park and Lam, Sluwa M.},  
journal={IEEE Transactions on Knowledge and Data Engineering},   
title={Generalized Analytic Rule Extraction for feedforward neural networks},   
year={1999},  
volume={11},  
number={6},  
pages={985-991},  
doi={10.1109/69.824621},
abstract={We suggest the Input-Network-Training-Output-Extraction-Knowledge framework to classify existing rule extraction algorithms for feedforward neural networks. Based on the suggested framework, we identify the major practices of existing algorithms as relying on the technique of generate and test, which leads to exponential complexity, relying on specialized network structure and training algorithms, which leads to limited applications and reliance on the interpretation of hidden nodes, which leads to proliferation of classification rules and their incomprehensibility. In order to generalize the applicability of rule extraction, we propose the rule extraction algorithm Generalized Analytic Rule Extraction (GLARE), and demonstrate its efficacy by comparing it with neural networks per se and the popular rule extraction program for decision trees, C4.5.},
keywords={type: classification, neural_network, rule_extraction}
}

@article{krishnan1999,
title = {A search technique for rule extraction from trained neural networks},
journal = {Pattern Recognition Letters},
volume = {20},
number = {3},
pages = {273-280},
year = {1999},
issn = {0167-8655},
doi = {https://doi.org/10.1016/S0167-8655(98)00145-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167865598001457},
author = {R. Krishnan and G. Sivakumar and P. Bhattacharya},
keywords = {type: neural_networks, rule_extraction, knowledge_discovery},
abstract = {Search methods for rule extraction from neural networks work by finding those combinations of inputs that make the neuron active. By sorting the input weights to a neuron and ordering the weights suitably, it is possible to prune the search space. Based on this observation, we present an algorithm for rule extraction from feedforward neural networks with boolean inputs and analyze its properties.}
}

@InProceedings{bologna2000,
author={Bologna, Guido},
editor={Wermter, Stefan
and Sun, Ron},
title={Symbolic Rule Extraction from the DIMLP Neural Network},
booktitle={Hybrid Neural Systems},
year={2000},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={240--254},
abstract={The interpretation of neural network responses as symbolic rules is actually a difficult task. Our first approach consists in characterising the discriminant hyper-plane frontiers. More particularly, we point out that the shape of a discriminant frontier built by a standard multi-layer perceptron is related to an equation with two terms. The first one is linear, and the second is logarithmic.},
isbn={978-3-540-46417-4},
%keywords={}
}

@InProceedings{kim200,
author={Kim, Hyeoncheol},
editor={Arikawa, Setsuo
and Morishita, Shinichi},
title={Computationally Efficient Heuristics for If-Then Rule Extraction from Feed-Forward Neural Networks},
booktitle={Discovery Science},
year={2000},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={170--182},
doi={10.1007/3-540-44418-1_14},
abstract={In this paper, we address computational complexity issues of decompositional approaches to if-then rule extraction from feed-forward neural networks. We also introduce a computationally efficient technique based on ordered-attributes. It reduces search space significantly and finds valid and general rules for single nodes in the networks. Empirical results are shown.},
isbn={978-3-540-44418-3},
%keywords={}
}

@article{setiono2000,
  title={FERNN: An Algorithm for Fast Extraction of Rules from Neural Networks},
  author={Setiono, Rudy and Leow, Wee Kheng},
  journal={Applied Intelligence},
  year={2000},
  volume={12},
  pages={15-25},
  abstract={Before symbolic rules are extracted from a trained neural network, the network is usually pruned so as to obtain more concise rules. Typical pruning algorithms require retraining the network which incurs additional cost. This paper presents FERNN, a fast method for extracting rules from trained neural networks without network retraining. Given a fully connected trained feedforward network with a single hidden layer, FERNN first identifies the relevant hidden units by computing their information gains. For each relevant hidden unit, its activation values is divided into two subintervals such that the information gain is maximized. FERNN finds the set of relevant network connections from the input units to this hidden unit by checking the magnitudes of their weights. The connections with large weights are identified as relevant. Finally, FERNN generates rules that distinguish the two subintervals of the hidden activation values in terms of the network inputs. Experimental results show that the size and the predictive accuracy of the tree generated are comparable to those extracted by another method which prunes and retrains the network.},
  doi={10.1023/A:1008307919726},
  keywords={type= rule_extraction, penalty_function, MofN_rule, DNF_rule, decision_tree}
}


@ARTICLE{tsukimoto2000,
  author={Tsukimoto, Hiroshi},
  journal={IEEE Transactions on Neural Networks}, 
  title={Extracting rules from trained neural networks}, 
  year={2000},
  volume={11},
  number={2},
  pages={377-389},
  doi={10.1109/72.839008},
  abstract={Presents an algorithm for extracting rules from trained neural networks. The algorithm is a decompositional approach which can be applied to any neural network whose output function is monotone such as a sigmoid function. Therefore, the algorithm can be applied to multilayer neural networks, recurrent neural networks and so on. It does not depend on training algorithms, and its computational complexity is polynomial. The basic idea is that the units of neural networks are approximated by Boolean functions. But the computational complexity of the approximation is exponential, and so a polynomial algorithm is presented. The author has applied the algorithm to several problems to extract understandable and accurate rules. The paper shows the results for the votes data, mushroom data, and others. The algorithm is extended to the continuous domain, where extracted rules are continuous Boolean functions. Roughly speaking, the representation by continuous Boolean functions means the representation using conjunction, disjunction, direct proportion, and reverse proportion. This paper shows the results for iris data.},
  keywords={type= boolean_functions, continuous_boolean_functions, multilinear_functions, neural_networks, rule_extraction}
}

@INPROCEEDINGS{zhou2000,
  author={Zhi-Hua Zhou and Shi-Fu Chen and Zhao-Qian Chen},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium}, 
  title={A statistics based approach for extracting priority rules from trained neural networks}, 
  year={2000},
  volume={3},
  pages={401-406},
  doi={10.1109/IJCNN.2000.861337},
  abstract={In this paper, a statistics based approach named STARE (statistics-based rule extraction) that is designed to extract symbolic rules from trained neural networks is proposed. STARE deals with continuous attributes in a unique way so that not only different attributes could be discretized to different number of clusters but also unnecessary discretization could be avoided. STARE introduces statistics to the generation and evaluation of priority rules that have concise appearance. Since it is independent of the network architectures and training algorithms, STARE could be applied to diversified neural classifiers. Experimental results show that rules extracted via STARE are comprehensible, compact and accurate.},
  %keywords={}
}

@INPROCEEDINGS{milare2001,
  author={Milar{\'e}, Claudia Regina and de Carvalho, Andr{\'e} C.P.L.F. and Monard, Maria Carolina},
  booktitle={Proceedings Fourth International Conference on Computational Intelligence and Multimedia Applications. ICCIMA 2001}, 
  title={Extracting rules from neural networks using symbolic algorithms: preliminary results}, 
  year={2001},
  pages={384-388},
  doi={10.1109/ICCIMA.2001.970500},
  abstract={Although Artificial Neural Networks (ANNs) have been satisfactorily employed in several problems, such as clustering, pattern recognition, dynamic systems control and prediction, they still suffer from significant limitations. One of them is that the induced concept representation is not usually comprehensible to humans. Several techniques have been suggested to extract meaningful knowledge from trained ANNs. This paper proposes the use of symbolic learning algorithms, commonly used by the Machine Learning community, to extract symbolic representations from trained ANNs. The procedure proposed is similar to that used by the Trepan algorithm (Craven, 1996), which extracts comprehensible, symbolic representations (decision trees) from trained ANNs.},
  %keywords={}}

@INPROCEEDINGS{sato2001,  
author={Sato, Makoto and Tsukimoto, Hiroshi},  
booktitle={IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)},   
title={Rule extraction from neural networks via decision tree induction},   
year={2001},  
volume={3},  
pages={1870-1875},  
doi={10.1109/IJCNN.2001.938448},
abstract={Rule extraction from neural networks is the task for obtaining comprehensible descriptions that approximate the predictive behavior of neural networks. Rule-extraction algorithms are used for both interpreting neural networks and mining the relationship between input and output variables in data. This paper describes a new rule extraction algorithm that extracts rules that contain both continuous (real-valued) and discrete literals. This algorithm decomposes a neural network using decision trees and obtains production rules by merging the rules extracted from each tree. Results tested on the databases in UCI repository are presented.},
%keywords={}}

@InProceedings{palade2001,
author={Palade, Vasile
and Neagu, Daniel-Ciprian
and Patton, Ron J.},
editor={Reusch, Bernd},
title={Interpretation of Trained Neural Networks by Rule Extraction},
booktitle={Computational Intelligence. Theory and Applications},
year={2001},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={152--161},
abstract={The paper focuses on the problem of rule extraction from neural networks, with the aim of transforming the knowledge captured in a trained neural network into a familiar form for human user. The ultimate purpose for us is to develop human friendly shells for neural network based systems. In the first part of the paper it is presented an approach on extracting traditional crisp rules out of the neural networks, while the last part of the paper presents how to transform the neural network into a set of fuzzy rules using an interactive fuzzy operator. The rules are extracted from ordinary neural networks, which have not a structure that facilitate the rule extraction. The neural network trained with the well known Iris data set was considered as benchmark problem.},
isbn={978-3-540-45493-9},
doi={10.1007/3-540-45493-4_20},
%keywords={}
}

@article{garcez2001,
title = {Symbolic knowledge extraction from trained neural networks: A sound approach},
journal = {Artificial Intelligence},
volume = {125},
number = {1},
pages = {155-207},
year = {2001},
issn = {0004-3702},
doi = {10.1016/S0004-3702(00)00077-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000771},
author = {Artur S. {d'Avila Garcez} and Krysia Broda and Dov M. Gabbay},
keywords = {type: neural_symbolic_integration, rule_extraction, nonmonotonic_reasoning, artificial_neural_networks},
abstract = {Although neural networks have shown very good performance in many application domains, one of their main drawbacks lies in the incapacity to provide an explanation for the underlying reasoning mechanisms. The “explanation capability” of neural networks can be achieved by the extraction of symbolic knowledge. In this paper, we present a new method of extraction that captures nonmonotonic rules encoded in the network, and prove that such a method is sound. We start by discussing some of the main problems of knowledge extraction methods. We then discuss how these problems may be ameliorated. To this end, a partial ordering on the set of input vectors of a network is defined, as well as a number of pruning and simplification rules. The pruning rules are then used to reduce the search space of the extraction algorithm during a pedagogical extraction, whereas the simplification rules are used to reduce the size of the extracted set of rules. We show that, in the case of regular networks, the extraction algorithm is sound and complete. We proceed to extend the extraction algorithm to the class of non-regular networks, the general case. We show that non-regular networks always contain regularities in their subnetworks. As a result, the underlying extraction method for regular networks can be applied, but now in a decompositional fashion. In order to combine the sets of rules extracted from each subnetwork into the final set of rules, we use a method whereby we are able to keep the soundness of the extraction algorithm. Finally, we present the results of an empirical analysis of the extraction system, using traditional examples and real-world application problems. The results have shown that a very high fidelity between the extracted set of rules and the network can be achieved.}
}

@ARTICLE{setiono2002,
  author={Setiono, Rudy  and Leow, Wee K. and Zurada, Jacek M.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Extraction of rules from artificial neural networks for nonlinear regression}, 
  year={2002},
  volume={13},
  number={3},
  pages={564-577},
  doi={10.1109/TNN.2002.1000125},
abstract={Neural networks (NNs) have been successfully applied to solve a variety of application problems including classification and function approximation. They are especially useful as function approximators because they do not require prior knowledge of the input data distribution and they have been shown to be universal approximators. In many applications, it is desirable to extract knowledge that can explain how Me problems are solved by the networks. Most existing approaches have focused on extracting symbolic rules for classification. Few methods have been devised to extract rules from trained NNs for regression. This article presents an approach for extracting rules from trained NNs for regression. Each rule in the extracted rule set corresponds to a subregion of the input space and a linear function involving the relevant input attributes of the data approximates the network output for all data samples in this subregion. Extensive experimental results on 32 benchmark data sets demonstrate the effectiveness of the proposed approach in generating accurate regression rules.},
keyword={type: network_pruning, regression, rule_extraction}}

@article{saito2002,
title = {Extracting regression rules from neural networks},
journal = {Neural Networks},
volume = {15},
number = {10},
pages = {1279-1288},
year = {2002},
issn = {0893-6080},
doi = {10.1016/S0893-6080(02)00089-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608002000898},
author = {Kazumi Saito and Ryohei Nakano},
keywords = {type: polynomial_equations, nominal_variables, regression_rules, rule_extraction, vector_quantization, Cross-validation, decision_trees},
abstract = {This paper proposes a new framework and method for extracting regression rules from neural networks trained with multivariate data containing both nominal and numeric variables. Each regression rule is expressed as a pair of a logical formula on the conditional part over nominal variables and a polynomial equation on the action part over numeric variables. The proposed extraction method first generates one such regression rule for each training sample, then utilizes the k-means algorithm to generate a much smaller set of rules having more general conditions, where the number of distinct polynomial equations is determined through cross-validation. Finally, this method invokes decision-tree induction to form logical formulae of nominal conditions as conditional parts of final regression rules. Experiments using four data sets show that our method works well in extracting quite accurate and interesting regression rules.}
}

@inproceedings{johansson2004,
  title={The truth is in there-rule extraction from opaque models using genetic programming.},
  author={Johansson, Ulf and K{\"o}nig, Rikard and Niklasson, Lars},
  booktitle={Proceedings of the FLAIRS Conference},
  pages={658--663},
  year={2004},
  url={https://www.aaai.org/Library/FLAIRS/2004/flairs04-113.php},
  abstract={A common problem whenusing complicated models for prediction and classification is that the complexity of the model entails that it is hard, or impossible, to interpret. For some scenarios this might not be a limitation, since the priority is the accuracy of the model. In other situations the limitations might be severe, since additional aspects are important to consider; e.g. comprehensibility or scalability of the model. In this study we show how the gap between accuracy and other aspects can be bridged by using a rule extraction method (termed G-REX) based on genetic programming. The extraction method is evaluated against the five criteria accuracy, comprehensibility, fidelity, scalability and generality. It is also shown how G-REX can create novel representation languages; here regression trees and fuzzy rules. The problem used is a data-mining problem from the marketing domain where the impact of advertising is predicted from investment plans. Several experiments, covering both regression and classification tasks, are evaluated. Results show that G-REX in general is capable of extracting both accurate and comprehensible representations, thus allowing high performance also in domains where comprehensibility is of essence.},
  %keyword={}
}

@InProceedings{lofstrom2004,
author={L{\"o}fstr{\"o}m, Tuve
and Johansson, Ulf
and Niklasson, Lars},
editor={Pal, Nikhil Ranjan
and Kasabov, Nik
and Mudi, Rajani K.
and Pal, Srimanta
and Parui, Swapan Kumar},
title={Rule Extraction by Seeing Through the Model},
booktitle={Neural Information Processing},
year={2004},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={555--560},
abstract={Much effort has been spent during recent years to develop techniques for rule extraction from opaque models, typically trained neural networks. A rule extraction technique could use different strategies for the extraction phase, either a local or a global strategy. The main contribution of this paper is the suggestion of a novel rule extraction method, called Cluster and See Through (CaST), based on the global strategy. CaST uses parts of the well-known RX algorithm, which is based on the local strategy, but in a slightly modified way. The novel method is evaluated against RX and is shown to get as good as or better results on all problems evaluated, with much more compact rules.},
isbn={978-3-540-30499-9},
doi={10.1007/978-3-540-30499-9_85},
%keywords={}
}

@article{setiono2004,
  title={An approach to generate rules from neural networks for regression problems},
  author={Setiono, Rudy and Thong, James YL},
  journal={European Journal of Operational Research},
  volume={155},
  number={1},
  pages={239--250},
  year={2004},
  publisher={HKUST Business School Research Paper No. 2021-005},
  url={https://ssrn.com/abstract=3766019},
  abstrac={Artificial neural networks have been successfully applied to a variety of business application problems involving classification and regression. They are especially useful for regression problems as they do not require prior knowledge about the data distribution. In many applications, it is desirable to extract knowledge from trained neural networks so that the users can gain a better understanding of the solution. Existing research works have focused primarily on extracting symbolic rules for classification problems with few methods devised for regression problems. In order to fill this gap, we propose an approach to extract rules from neural networks that have been trained to solve regression problems. The extracted rules divide the data samples into groups. For all samples within a group, a linear function of the relevant input attributes of the data approximates the network output. The approach is illustrated with two examples on various application problems. Experimental results show that the proposed approach generates rules that are more accurate than the existing methods based on decision trees and linear regression.},
  keywords={type: neural_networks, nonlinear_regression, curve_fitting, machine_learning, knowledge-based_systems}
}

@article{kamruzzaman2010,
  doi = {10.48550/ARXIV.1009.4570},
  url = {https://arxiv.org/abs/1009.4570},
  author = {Kamruzzaman, S. M. and Islam, Md. Monirul},
  keywords = {type: backpropagation, clustering_algorithm, constructive_algorithm, continuous_activation_function, pruning_algorithm, rule_extraction_algorithm, symbolic_rules},
  title = {Extraction of Symbolic Rules from Artificial Neural Networks},
  publisher = {arXiv},
  year = {2010},
  copyright = {arXiv.org perpetual, non-exclusive license},
  abstract={Although backpropagation ANNs generally predict better than decision trees do for pattern classification problems, they are often regarded as black boxes, i.e., their predictions cannot be explained as those of decision trees. In many applications, it is desirable to extract knowledge from trained ANNs for the users to gain a better understanding of how the networks solve the problems. A new rule extraction algorithm, called rule extraction from artificial neural networks (REANN) is proposed and implemented to extract symbolic rules from ANNs. A standard three-layer feedforward ANN is the basis of the algorithm. A four-phase training algorithm is proposed for backpropagation learning. Explicitness of the extracted rules is supported by comparing them to the symbolic rules generated by other methods. Extracted rules are comparable with other methods in terms of number of rules, average number of conditions for a rule, and predictive accuracy. Extensive experimental studies on several benchmarks classification problems, such as breast cancer, iris, diabetes, and season classification problems, demonstrate the effectiveness of the proposed approach with good generalization ability.},
}


@article{lu2006,
  title={Explanatory rule extraction based on the trained neural network and the genetic programming},
  author={Lu, Jianjun and Tokinaga, Shozo and Ikeda, Yoshikazu},
  journal={Journal of the Operations Research Society of Japan},
  volume={49},
  number={1},
  pages={66-82},
  year={2006},
  doi={10.15807/jorsj.49.66},
  abstract={This paper deals with the use of neural network rule extraction techniques based on the Genetic Programming (GP) to build intelligent and explanatory evaluation systems. Recent development in algorithms that extract rules from trained neural networks enable us to generate classification rules in spite of their intrinsically black-box nature. However, in the original decompositional method looking at the internal structure of the networks, the comprehensive methods combining the output to the inputs using parameters are complicated. Then, in our paper, we utilized the GP to automatize the rule extraction process in the trained neural networks where the statements changed into a binary classification. Even though the production (classification) rule generation based on the GP alone are applicable straightforward to the underlying problems for decision making, but in the original GP method production rules include many statements described by arithmetic expressions as well as basic logical expressions, and it makes the rule generation process very complicated. Therefore, we utilize the neural network and binary classification to obtain simple and relevant classification rules in real applications by avoiding straightforward applications of the GP procedure to the arithmetic expressions. At first, the pruning process of weight among neurons is applied to obtain simple but substantial binary expressions which are used as statements is classification rules. Then, the GP is applied to generate ultimate rules. As applications, we generate rules to prediction of bankruptcy and creditworthiness for binary classifications, and the apply the method to multi-level classification of corporate bonds (rating) by using the financial indicators.},
  keywords={type: algorithm, rule_extraction, neural_networks, genetic_programming, prediction_of_bankruptcy, bond_rating}
}


@article{Hruschka 2006,
title = {Extracting rules from multilayer perceptrons in classification problems: A clustering-based approach},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {384-397},
year = {2006},
issn = {0925-2312},
doi = {10.1016/j.neucom.2005.12.127},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206000403},
author = {Hruschka, Eduardo R. and Ebecken, Nelson F.F.},
keywords = {type: rule_extraction_from_neural_networks, clustering, genetic_algorithms},
abstract = {Multilayer perceptrons adjust their internal parameters performing vector mappings from the input to the output space. Although they may achieve high classification accuracy, the knowledge acquired by such neural networks is usually incomprehensible for humans. This fact is a major obstacle in data mining applications, in which ultimately understandable patterns (like classification rules) are very important. Therefore, many algorithms for rule extraction from neural networks have been developed. This work presents a method to extract rules from multilayer perceptrons trained in classification problems. The rule extraction algorithm basically consists of two steps. First, a clustering genetic algorithm is applied to find clusters of hidden unit activation values. Then, classification rules describing these clusters, in relation to the inputs, are generated. The proposed approach is experimentally evaluated in four datasets that are benchmarks for data mining applications and in a real-world meteorological dataset, leading to interesting results.}
}

@article{kaikhah2006,
  title={Discovering trends in large datasets using neural networks},
  author={Kaikhah, Khosrow and Doddameti, Sandesh},
  journal={Applied Intelligence},
  volume={24},
  pages={51--60},
  year={2006},
  doi={10.1007/s10489-006-6929-9},
  publisher={Springer},
  abstract={A novel knowledge discovery technique using neural networks is presented. A neural network is trained to learn the correlations and relationships that exist in a dataset. The neural network is then pruned and modified to generalize the correlations and relationships. Finally, the neural network is used as a tool to discover all existing hidden trends in four different types of crimes (murder, rape, robbery, and auto theft) in US cities as well as to predict trends based on existing knowledge inherent in the network.},
  %keywords={}
}

@INPROCEEDINGS{odajima2006,
  author={Odajima, Koichi and Hayashi, Yoichi and Setiono, Rudy},
  booktitle={The 2006 IEEE International Joint Conference on Neural Network Proceedings}, 
  title={Greedy rule generation from discrete data and its use in neural network rule extraction}, 
  year={2006},
  pages={1833-1839},
  doi={10.1109/IJCNN.2006.246902}
  abstract={This paper proposes GRG (greedy rule generation) algorithm for generating classification rules from a data set with discrete attributes. The algorithm is "greedy" in the sense that at every iteration, it searches for the best rule to generate. The criteria for the best rule include the number of samples that it covers, the number of attributes involved in the rule, and the size of the input subspace it covers. This method is applied for extracting rules from neural networks that have been trained and pruned for solving classification problems. Neural networks with one hidden layer are trained and the proposed GRG algorithm is applied to their discretized hidden unit activation values. Our results show that rule extraction with the GRG method produces rule sets that are more accurate and concise compared to those obtained by a decision tree method and an existing neural network rule extraction method.},
  %keywords={}
}

@InProceedings{obzakir2008,
author={{\"O}zbak{\i}r, Lale
and Baykaso{\u{g}}lu, Adil
and Kulluk, Sinem},
editor={Maniezzo, Vittorio
and Battiti, Roberto
and Watson, Jean-Paul},
title={Rule Extraction from Neural Networks Via Ant Colony Algorithm for Data Mining Applications},
booktitle={Learning and Intelligent Optimization},
year={2008},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={177--191},
abstract={A common problem in Data Mining (DM) is the presence of noise in the data being mined. Artificial neural networks (ANN) are robust and have a good tolerance to noise, which makes them suitable for mining very noisy data. Although they may achieve high classification accuracy, they have the well-known disadvantage of having black-box nature and not discovering any high-level rule that can be used as a support for human understanding. The main challenge in using ANN in DM applications is to get explicit knowledge from these models. For this purpose, a study on knowledge acquirement from trained ANNs for classification problems is presented. The proposed method uses Touring Ant Colony Optimization (TACO) algorithm for extracting accurate and comprehensible rules from databases via trained artificial neural networks. The suggested algorithm is experimentally evaluated on different benchmark data sets. Results show that the proposed approach has a potential to generate accurate and concise rules.},
isbn={978-3-540-92695-5},
doi={10.1007/978-3-540-92695-5_14},
keywords={type: data_mining, artificial_neural_networks, ant_colony_optimization, classification_rules}
}

@ARTICLE{setiono2008,
  author={Setiono, Rudy and Baesens, Bart and Mues, Christophe},
  journal={IEEE Transactions on Neural Networks}, 
  title={Recursive Neural Network Rule Extraction for Data With Mixed Attributes}, 
  year={2008},
  volume={19},
  number={2},
  pages={299-307},
  doi={10.1109/TNN.2007.908641},
  abstract={In this paper, we present a recursive algorithm for extracting classification rules from feedforward neural networks (NNs) that have been trained on data sets having both discrete and continuous attributes. The novelty of this algorithm lies in the conditions of the extracted rules: the rule conditions involving discrete attributes are disjoint from those involving continuous attributes. The algorithm starts by first generating rules with discrete attributes only to explain the classification process of the NN. If the accuracy of a rule with only discrete attributes is not satisfactory, the algorithm refines this rule by recursively generating more rules with discrete attributes not already present in the rule condition, or by generating a hyperplane involving only the continuous attributes. We show that for three real-life credit scoring data sets, the algorithm generates rules that are not only more accurate but also more comprehensible than those generated by other NN rule extraction methods.},
  keywords={type: continuous_attributes, credit_scoring, discrete_attributes, rule_extraction}
}


@article{ozbakir2009,
title = {TACO-miner: An ant colony based algorithm for rule extraction from trained neural networks},
journal = {Expert Systems with Applications},
volume = {36},
number = {10},
pages = {12295-12305},
year = {2009},
issn = {0957-4174},
doi = {10.1016/j.eswa.2009.04.058},
url = {https://www.sciencedirect.com/science/article/pii/S0957417409004084},
author = {{\"O}zbak{\i}r, Lale and Baykaso{\u{g}}lu, Adil and Kulluk, Sinem and Yapıcı, H{\"u}seyin },
keywords = {type: data_mining, artificial_neural_networks, ant_colony_optimization, classification_rules},
abstract = {Extracting classification rules from data is an important task of data mining and gaining considerable more attention in recent years. In this paper, a new meta-heuristic algorithm which is called as TACO-miner is proposed for rule extraction from artificial neural networks (ANN). The proposed rule extraction algorithm actually works on the trained ANNs in order to discover the hidden knowledge which is available in the form of connection weights within ANN structure. The proposed algorithm is mainly based on a meta-heuristic which is known as touring ant colony optimization (TACO) and consists of two-step hierarchical structure. The proposed algorithm is experimentally evaluated on six binary and n-ary classification benchmark data sets. Results of the comparative study show that TACO-miner is able to discover accurate and concise classification rules.}
}


@article{kahramanli2009,
title = {Rule extraction from trained adaptive neural networks using artificial immune systems},
journal = {Expert Systems with Applications},
volume = {36},
number = {2, Part 1},
pages = {1513-1522},
year = {2009},
issn = {0957-4174},
doi = {10.1016/j.eswa.2007.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0957417407005878},
author = {Kahramanli, Humar and Allahverdi, Novruz},
keywords = {type: adaptive_neural_networks, artificial_immune_systems, optimization, rule_extraction, backpropagation, opt-ainet},
abstract = {Although artificial neural network (ANN) usually reaches high classification accuracy, the obtained results sometimes may be incomprehensible. This fact is causing a serious problem in data mining applications. The rules that are derived from ANN are needed to be formed to solve this problem and various methods have been improved to extract these rules. Activation function is critical as the behavior and performance of an ANN model largely depends on it. So far there have been limited studies with emphasis on setting a few free parameters in the neuron activation function. ANN’s with such activation function seem to provide better fitting properties than classical architectures with fixed activation function neurons [Xu, S., & Zhang, M. (2005). Data mining – An adaptive neural network model for financial analysis. In Proceedings of the third international conference on information technology and applications]. In this study a new method that uses artificial immune systems (AIS) algorithm has been presented to extract rules from trained adaptive neural network. Two real time problems data were investigated for determining applicability of the proposed method. The data were obtained from University of California at Irvine (UCI) machine learning repository. The datasets were obtained from Breast Cancer disease and ECG data. The proposed method achieved accuracy values 94.59% and 92.31% for ECG and Breast Cancer dataset, respectively. It has been observed that these results are one of the best results comparing with results obtained from related previous studies and reported in UCI web sites.}
}

@INPROCEEDINGS{huynh2009,
  author={Huynh, Thuan Q. and Reggia, James A.},
  booktitle={2009 International Joint Conference on Neural Networks}, 
  title={Improving rule extraction from neural networks by modifying hidden layer representations}, 
  year={2009},
  pages={1316-1321},
  doi={10.1109/IJCNN.2009.5178685},
  abstract={This paper describes a new method for extracting symbolic rules from multilayer feedforward neural networks. Our approach is to encourage backpropagation to learn a sparser representation at the hidden layer and to use the improved representation to extract fewer, easier to understand rules. A new error term defined over the hidden layer is added to the standard sum of squared error so that the total squared distance between hidden activation vectors is increased. We show that this method helps extract fewer rules without decreasing classification accuracy in four publicly available data sets.},
  %keywords={} 
}

@article{ozbakir2010,
title = {A soft computing-based approach for integrated training and rule extraction from artificial neural networks: DIFACONN-miner},
journal = {Applied Soft Computing},
volume = {10},
number = {1},
pages = {304-317},
year = {2010},
issn = {1568-4946},
doi = {10.1016/j.asoc.2009.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1568494609001318},
author = {{\"O}zbak{\i}r, Lale and Baykaso{\u{g}}lu, Adil and Kulluk, Sinem},
keywords = {type: data_mining, rule_extraction, classification, artificial_neural_networks, differential_evolution, ant_colony_optimization},
abstract = {Artificial neural network (ANN) is one of the most widely used techniques in classification data mining. Although ANNs can achieve very high classification accuracies, their explanation capability is very limited. Therefore one of the main challenges in using ANNs in data mining applications is to extract explicit knowledge from them. Based on this motivation, a novel approach is proposed in this paper for generating classification rules from feed forward type ANNs. Although there are several approaches in the literature for classification rule extraction from ANNs, the present approach is fundamentally different from them. In the previous studies, ANN training and rule extraction is generally performed independently in a sequential (hierarchical) manner. However, in the present study, training and rule extraction phases are integrated within a multiple objective evaluation framework for generating accurate classification rules directly. The proposed approach makes use of differential evolution algorithm for training and touring ant colony optimization algorithm for rule extracting. The proposed algorithm is named as DIFACONN-miner. Experimental study on the benchmark data sets and comparisons with some other classical and state-of-the art rule extraction algorithms has shown that the proposed approach has a big potential to discover more accurate and concise classification rules.}
}

@article{augasta2012,
  title={Reverse engineering the neural networks for rule extraction in classification problems},
  author={Augasta, M. Gethsiyal and Kathirvalavakumar, Thangairulappan},
  journal={Neural processing letters},
  volume={35},
  pages={131--150},
  year={2012},
  publisher={Springer}
  doi={10.1007/s11063-011-9207-8},
  abstract={Artificial neural networks often achieve high classification accuracy rates, but they are considered as black boxes due to their lack of explanation capability. This paper proposes the new rule extraction algorithm RxREN to overcome this drawback. In pedagogical approach the proposed algorithm extracts the rules from trained neural networks for datasets with mixed mode attributes. The algorithm relies on reverse engineering technique to prune the insignificant input neurons and to discover the technological principles of each significant input neuron of neural network in classification. The novelty of this algorithm lies in the simplicity of the extracted rules and conditions in rule are involving both discrete and continuous mode of attributes. Experimentation using six different real datasets namely iris, wbc, hepatitis, pid, ionosphere and creditg show that the proposed algorithm is quite efficient in extracting smallest set of rules with high classification accuracy than those generated by other neural network rule extraction methods.},
  keywords={type: rule_extraction, pedagogical, reverse_engineering, classification, pruning, neural_networks}
}

@INPROCEEDINGS{sethi2012,
  author={Sethi, Kamal Kumar and Mishra, Durgesh Kumar and Mishra, Bharat},
  booktitle={2012 Third International Conference on Intelligent Systems Modelling and Simulation}, 
  title={KDRuleEx: A Novel Approach for Enhancing User Comprehensibility Using Rule Extraction}, 
  year={2012},
  pages={55-60},
  doi={10.1109/ISMS.2012.116},
  abstract={Knowledge representation opaque model like ANN has advantage of accuracy and limitation of interpretability. Transparent models like decision tree, decision table and rules represent knowledge in more understandable form and can easily be integrated with other learning system. Converting an opaque model like ANN to transparent model is called rule extraction. Lot of work has been done in the field of rule extraction like "SVM to Rules", "ANN to Decision Tree", "ANN to Rules" etc. We did not find any direct approach of converting ANN to decision table in our literature survey. In this paper, we proposed a novel pedagogical rule extraction technique to generate a decision table using training example set and a trained artificial neural network on it. The proposed algorithm can be used with both discrete as well as continuous input. The proposed algorithm has advantage in terms of computational performance and memory management.},
  keywords={rule_extraction, decision_table, ann, fidelity, accuracy, comprehensibility.}
}

@misc{singh2016,
  doi = {10.48550/ARXIV.1611.07579}, 
  url = {https://arxiv.org/abs/1611.07579},
  author = {Singh, Sameer and Ribeiro, Marco Tulio and Guestrin, Carlos},
  %keywords = {},
  title = {Programs as Black-Box Explanations},
  journal={arXiv preprint arXiv:1611.07579},
  year = {2016},
  %abstract{}
}

@InProceedings{zilke2016,
author={Zilke, Jan Ruben
and Loza Menc{\'i}a, Eneldo
and Janssen, Frederik},
editor={Calders, Toon
and Ceci, Michelangelo
and Malerba, Donato},
title={DeepRED -- Rule Extraction from Deep Neural Networks},
booktitle={Discovery Science},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={457--473},
abstract={Neural network classifiers are known to be able to learn very accurate models. In the recent past, researchers have even been able to train neural networks with multiple hidden layers (deep neural networks) more effectively and efficiently. However, the major downside of neural networks is that it is not trivial to understand the way how they derive their classification decisions. To solve this problem, there has been research on extracting better understandable rules from neural networks. However, most authors focus on nets with only one single hidden layer. The present paper introduces a new decompositional algorithm -- DeepRED -- that is able to extract rules from deep neural networks.},
isbn={978-3-319-46307-0},
doi={10.1007/978-3-319-46307-0_29},
%keywords={}
}

@article{thiagarajan2016,
  title={Treeview: Peeking into deep neural networks via feature-space partitioning},
  author={Thiagarajan, Jayaraman J and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
  journal={arXiv preprint arXiv:1611.07429},
  year={2016},
  abstract={With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior \cite{mythos}. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted. },
  url={https://arxiv.org/abs/1611.07429},
  %keywords={}
}

@article{uzun2016,
  title={Rule extraction from training artificial neural network using variable neighbourhood search for Wisconsin Breast Cancer},
  author={UZUN, Yusuf and ARIKAN, H{\"u}seyin and TEZEL, G{\"u}lay},
  journal={Journal of Multidisciplinary Engineering Science and Technology},
  volume={3},
  number={8},
  year={2016},
  abstract={Artificial neural network (ANN) may achieve successful classification value, but obtained results sometimes can be unintelligible and may not be interpreted. In literature, different rule extraction methods from trained ANN are applied to overcome this problem. This study presented a new method of extraction correct and intelligible rules from Trained ANN using variable neighbourhood search (VNS) metaheuristic method. Since the VNS method is independent from the ANN, it does not modify the results of ANN. Real time Wisconsin Breast Cancer (WBC) data was examined for determining the feasibility of the suggested method. The VNS method was used to obtain the best fitness values belong to input attributes, Id, which was maximized the fitness function Sr of output node r. The suggested method was the computational evaluated on investigated data sets and obtained results from the suggested approach indicated that had a potential to produce accurately and cored rules. The proposed method obtained accuracy value 98.97% for WBC dataset. },
  url={http://www.jmest.org/wp-content/uploads/JMESTN42351743.pdf},
  keywords={type: variable_neighbourhood_search, artificial_neural_network, optimization}
}

@inproceedings{che2016,
  title={Interpretable deep models for ICU outcome prediction},
  author={Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
  booktitle={AMIA annual symposium proceedings},
  volume={2016},
  pages={371--380},
  year={2016},
  abstract={Exponential surge in health care data, such as longitudinal data from electronic health records (EHR), sensor data from intensive care unit (ICU), etc., is providing new opportunities to discover meaningful data-driven characteristics and patterns ofdiseases. Recently, deep learning models have been employedfor many computational phenotyping and healthcare prediction tasks to achieve state-of-the-art performance. However, deep models lack interpretability which is crucial for wide adoption in medical research and clinical decision-making. In this paper, we introduce a simple yet powerful knowledge-distillation approach called interpretable mimic learning, which uses gradient boosting trees to learn interpretable models and at the same time achieves strong prediction performance as deep learning models. Experiment results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed method not only outperforms state-of-the-art approaches for morality and ventilator free days prediction tasks but can also provide interpretable models to clinicians.},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333206/},
  %keywords={}
}







@article{biswas2017,
author = {Biswas, Saroj Kumar and Chakraborty, Manomita and Purkayastha, Biswajit and Roy, Pinki and Thounaojam, Dalton Meitei},
title = {Rule Extraction from Training Data Using Neural Network},
journal = {International Journal on Artificial Intelligence Tools},
volume = {26},
number = {03},
pages = {1750006},
year = {2017},
doi = {10.1142/S0218213017500063},
url = {https://doi.org/10.1142/S0218213017500063},
eprint = {https://doi.org/10.1142/S0218213017500063},
abstract = { Data Mining is a powerful technology to help organization to concentrate on most important data by extracting useful information from large database. One of the most commonly used techniques in data mining is Artificial Neural Network due to its high performance in many application domains. Despite many advantages of Artificial Neural Network, one of its main drawbacks is its inherent black box nature which is the main problem of using Artificial Neural Network in data mining. Therefore, this paper proposes a rule extraction algorithm from neural network using classified and misclassified data to convert the black box nature of Artificial Neural Network into a white box. The proposed algorithm is a modification of the existing algorithm, Rule Extraction by Reverse Engineering (RxREN). The proposed algorithm extracts rules from trained neural network for datasets with mixed mode attributes using pedagogical approach. The proposed algorithm uses both classified as well as misclassified data to find out the data ranges of significant attributes in respective classes, which is the innovation of the proposed algorithm. The experimental results clearly show that the performance of the proposed algorithm is superior to existing algorithms.},
keywords={type: data_mining, artificial_neural_networks, rule_extraction, pedagogical, rxren_algorithm, classification.}
}

@inproceedings{frosst2017,
title= {Distilling a Neural Network Into a Soft Decision Tree},
booktitle={CEX workshop at AI*IA 2017 conference},
author= {Frosst, Nicholas and Hinton, Geoffrey},
year={2017},
url={https://arxiv.org/pdf/1711.09784.pdf},
abstract{Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
%keywords={}
}

@article{bondarenko2017,
title = {Classification Tree Extraction from Trained Artificial Neural Networks},
journal = {Procedia Computer Science},
volume = {104},
pages = {556-563},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {10.1016/j.procs.2017.01.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301734},
author = {Bondarenko, Andrey and Aleksejeva, Ludmila and Jumutc, Vilen and Borisov, Arkady},
keywords = {type: classification_decision_tree, knowledge_extraction, neural_networks},
abstract = {Recent advances in neural networks design and training provoked the 2nd artificial neural networks (ANN) renaissance. In many cases classification decision made by trained fully connected neural nets is better than that acquired by models like C4.5 or C5.01,2. But in contrast to decision trees, ANN models are “black boxes”, i.e., it is impossible to understand how classification decision is made. In many areas it is critical and even obligate to understand how a model performs classification thus rendering ANN usage as obsolete. Recently, some researchers have proposed and described separate steps that would allow extracting knowledge from a trained multi-layered fully connected sigmoidal neural network. This process involves several steps such as trained network training, pruning and knowledge extraction. This paper provides an overview of all the aforementioned steps, as well as describes how a knowledge extraction system can be built. We describe our Neural Network Knowledge eXtraction (NNKX) system and provide experimental results of rule extraction from the trained multi-layered feed-forward sigmoidal artificial neural network in the form of binary classification decision trees. The results obtained suggest that extracted decision trees have good classification accuracy and sizes comparable to C4.5 trees and even overcoming them in some cases. Thus the proposed system can be successfully applied to better understand and validate ANN models. We provide link to source code repository with the implementation of described system.}
}

@article{guidotti2018,
  title={Local rule-based explanations of black box decision systems},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  journal={arXiv preprint arXiv:1805.10820},
  year={2018},
abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. %Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. 
Wide experiments show that LORE outperforms existing methods and baselines both in the quality 
of explanations and in the accuracy in mimicking the black box.}, 
doi = {10.48550/ARXIV.1805.10820},
keywords={type: explanation, decision_systems, rules}
}


@misc{guidotti2018,
  abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. %Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. 
Wide experiments show that LORE outperforms existing methods and baselines both in the quality 
of explanations and in the accuracy in mimicking the black box.}, 
  
  url = {https://arxiv.org/abs/1805.10820},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Local Rule-Based Explanations of Black Box Decision Systems},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}







@article{biswas2018,
author = {Biswas, Saroj Kr. and Chakraborty, Manomita and Purkayastha, Biswajit},
title = {A rule generation algorithm from neural network using classified and misclassified data},
journal = {International Journal of Bio-Inspired Computation},
volume = {11},
number = {1},
pages = {60-70},
year = {2018},
doi = {10.1504/IJBIC.2018.090070},
url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJBIC.2018.090070},
eprint = {https://www.inderscienceonline.com/doi/pdf/10.1504/IJBIC.2018.090070},
abstract = {Classification is one of the important tasks of data mining and neural network is one of the best known tools for doing this task. Despite of producing high classification accuracy, the black box nature of neural network makes it useless for many applications which require transparency in its decision-making process. This drawback is overcome by extracting rules from neural network. Rule extraction makes neural network an alternative to other machine learning methods for handling classification problems by deriving an explanation of how each decision is made. Till now, many algorithms on rule extraction have been proposed but still research on this area is going on to find out more accurate and understandable rules. The proposed algorithm extracts rules from trained neural network for datasets with mixed mode attributes using pedagogical approach. The proposed algorithm uses classified and misclassified patterns to find out the data ranges of significant attributes in respective classes. The experimental results clearly show that the proposed algorithm produces accurate and understandable rules compared to existing algorithms. },
keyword={type: data_mining, artificial_neural_networks, anns, rule_extraction, pedagogical, RxREN_algorithm, classification}
}

@article{yedjour2018,
title = {Symbolic interpretation of artificial neural networks based on multiobjective genetic algorithms and association rules mining},
journal = {Applied Soft Computing},
volume = {72},
pages = {177-188},
year = {2018},
issn = {1568-4946},
doi = {10.1016/j.asoc.2018.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618304551},
author = {Yedjour, Dounia and Benyettou, Abdelkader},
keywords = {type: neural_networks, class_association_rules, genetic_algorithm, multiobjective_optimization},
abstract = {Rule extraction from neural networks is an important task. In practice, decision makers often settle for using less accurate, but comprehensible models, typically decision trees where the solutions are given in graphical form easily interpretable. The black-box rule extraction techniques, operating directly on the input-output relationship, are clearly superior to the restricted open-box methods, normally tailored for a specific architecture. This is especially important since most data miners today will use some kind of ensemble (instead of a single model) to maximize accuracy. Consequently, the ability to extract rules from any opaque model is a key demand for rule extraction techniques. This paper proposes a new multiobjective genetic method to extract knowledge from trained artificial neural network by using the association rules technique. The main aim of this hybridization is to extract the optimal rules from the neural network for further classification. The algorithm consists of two stages: the rule filtering phase which eliminates misleading rules by taking into account the support, the confidence and the lift measures, then, rule set optimization phase which finds the set of optimal rule sets by considering fidelity, coverage and complexity measures. The algorithm is evaluated on 05 UCI datasets. The experimental results show that the proposal provides interesting rules. Accuracy and comprehensibility are clearly improved, and subsequently, it can become a challengeable and trustful research field in the area of neural network rule extraction.}
}

@article{ribeiro2018, 
title={Anchors: High-Precision Model-Agnostic Explanations}, 
volume={32}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/11491}, 
doi={10.1609/aaai.v32i1.11491}, 
abstract={We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, &quot;sufficient&quot; conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.}, 
number={1}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos}, 
year={2018}, 
month={Apr.},
%keywords{}}

@inproceedings{wu2018,
  title={Beyond sparsity: Tree regularization of deep models for interpretability},
  author={Wu, Mike and Hughes, Michael and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018},
  abstract={The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.},
  doi={10.1609/aaai.v32i1.11501},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/11501},
  keywords{}
}

@INPROCEEDINGS{liu2018,  
author={Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},  
booktitle={2018 IEEE International Conference on Data Mining Workshops (ICDMW)},   
title={Improving the Interpretability of Deep Neural Networks with Knowledge Distillation},   
year={2018},  
pages={905-912},  
doi={10.1109/ICDMW.2018.00132},
abstract={Deep Neural Networks have achieved huge success at a wide spectrum of applications from language modeling, computer vision to speech recognition. However, nowadays, good performance alone is not enough to satisfy the needs of practical deployment where interpretability is demanded for cases involving ethics and mission critical applications. The complex models of Deep Neural Networks make it hard to understand and reason the predictions, which hinders its further progress. To tackle this problem, we apply the Knowledge Distillation technique to distill Deep Neural Networks into decision trees in order to attain good performance and interpretability simultaneously. We formulate the problem at hand as a multi-output regression problem and the experiments demonstrate that the student model achieves significantly better accuracy performance (about 1% to 5%) than vanilla decision trees at the same level of tree depth. The experiments are implemented on the TensorFlow platform to make it scalable to big datasets. To the best of our knowledge, we are the first to distill Deep Neural Networks into vanilla decision trees on multi-class datasets.},
keywords={type: interpretation, neural_networks, decision_tree, tensorFlow, dark_knowledge, knowledge_distillation}
}

@article{chakraborty2019,
  title={Rule extraction from neural network using input data ranges recursively},
  author={Chakraborty, Manomita and Biswas, Saroj Kumar and Purkayastha, Biswajit},
  journal={New Generation Computing},
  volume={37},
  pages={67--96},
  year={2019},
  publisher={Springer},
  abstract={Neural network is one of the best tools for data mining tasks due to its high accuracy. However, one of the drawbacks of neural network is its black box nature. This limitation makes neural network useless for many applications which require transparency in their decision-making process. Many algorithms have been proposed to overcome this drawback by extracting transparent rules from neural network, but still researchers are in search for algorithms that can generate more accurate and simple rules. Therefore, this paper proposes a rule extraction algorithm named Eclectic Rule Extraction from Neural Network Recursively (ERENNR), with the aim to generate simple and accurate rules. ERENNR algorithm extracts symbolic classification rules from a single-layer feed-forward neural network. The novelty of this algorithm lies in its procedure of analyzing the nodes of the network. It analyzes a hidden node based on data ranges of input attributes with respect to its output and analyzes an output node using logical combination of the outputs of hidden nodes with respect to output class. And finally it generates a rule set by proceeding in a backward direction starting from the output layer. For each rule in the set, it repeats the whole process of rule extraction if the rule satisfies certain criteria. The algorithm is validated with eleven benchmark datasets. Experimental results show that the generated rules are simple and accurate.},
  keywords={type: neural_network, data_mining, rule_extraction, classification, re-rx_algorithm, rxren_algorithm},
  doi={10.1007/s00354-018-0048-0}
}

@inproceedings{chattopadhyay2019,
  title={Neural network attributions: A causal perspective},
  author={Chattopadhyay, Aditya and Manupriya, Piyushi and Sarkar, Anirban and Balasubramanian, Vineeth N},
  booktitle={International Conference on Machine Learning},
  pages={981--990},
  year={2019},
  organization={PMLR},
  abstract={We propose a new attribution method for neural networks developed using first principles of causality (to the best of our knowledge, the first such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efficiently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.},

}


@InProceedings{chattopadhyay2019,
  title = 	 {Neural Network Attributions: A Causal Perspective},
  author =       {Chattopadhyay, Aditya and Manupriya, Piyushi and Sarkar, Anirban and Balasubramanian, Vineeth N},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {981--990},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/chattopadhyay19a/chattopadhyay19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/chattopadhyay19a.html},
  abstract = 	 {We propose a new attribution method for neural networks developed using ﬁrst principles of causality (to the best of our knowledge, the ﬁrst such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efﬁciently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.}?
  %keywords=	 {}
}

@inproceedings{mereani2019,
  title={Exact and approximate rule extraction from neural networks with Boolean features},
  author={Mereani, Fawaz A. and Howe, Jacob M.},
  booktitle={Proceedings of the 11th International Joint Conference on Computational Intelligence},
  pages={424--433},
  year={2019},
  organization={SCITEPRESS},
  abstract={Rule extraction from classifiers treated as black boxes is an important topic in explainable artificial intelligence (XAI). It is concerned with finding rules that describe classifiers and that are understandable to humans, having the form of (I f...T hen...Else). Neural network classifiers are one type of classifier where it is difficult to know how the inputs map to the decision. This paper presents a technique to extract rules from a neural network where the feature space is Boolean, without looking at the inner structure of the network. For such a network with a small feature space, a Boolean function describing it can be directly calculated, whilst for a network with a larger feature space, a sampling method is described to produce rule-based approximations to the behaviour of the network with varying granularity, leading to XAI. The technique is experimentally assessed on a dataset of cross-site scripting (XSS) attacks, and proves to give very high accuracy and precision, comparable to that given by the neural network being approximated.},
  keywords={type: neural_networks, xss, rule_extraction, explainable_ai},
  doi={10.5220/0008362904240433}
}

@article{chakraborty2020,
  title={Rule extraction from neural network trained using deep belief network and back propagation},
  author={Chakraborty, Manomita and Biswas, Saroj Kumar and Purkayastha, Biswajit},
  journal={Knowledge and Information Systems},
  volume={62},
  pages={3753--3781},
  year={2020},
  publisher={Springer},
  abstracte={Representing the knowledge learned by neural networks in the form of interpretable rules is a prudent technique to justify the decisions made by neural networks. Heretofore many algorithms exist to extract symbolic rules from neural networks, but among them, a few extract rules from deep neural networks trained using deep learning techniques. So, this paper proposes an algorithm to extract rules from a multi-hidden layer neural network, pre-trained using deep belief network and fine-tuned using back propagation. The algorithm analyzes each node of a layer and extracts knowledge from each layer separately. The process of knowledge extraction from the first hidden layer is different from the other layers. Consecutively, the algorithm combines all the knowledge extracted and refines them to construct a final ruleset consisting of symbolic rules. The algorithm further subdivides the subspace of a rule in the ruleset if it satisfies certain conditions. Results show that the algorithm extracted rules with higher accuracy compared to some existing rule extraction algorithms. Other than accuracy, the efficacy of the extracted rules is also validated with fidelity and various other performance measures. 
},
  keywords={type: neural_network, classification, rule_extraction, back_propagation, deep_learning, deep_belief_network, restricted_boltzmann_machine},
  doi={10.1007/s10115-020-01473-0}
}

@article{yedjour2020,
  title={Extracting classification rules from artificial neural network trained with discretized inputs},
  author={Yedjour, Dounia},
  journal={Neural Processing Letters},
  volume={52},
  pages={2469--2491},
  year={2020},
  publisher={Springer},
  abstract{Rule extraction from artificial neural networks remains important task in complex diseases such as diabetes and breast cancer where the rules should be accurate and comprehensible. The quality of rules is improved by the improvement of the network classification accuracy which is done by the discretization of input attributes. In this paper, we developed a rule extraction algorithm based on multiobjective genetic algorithms and association rules mining to extract highly accurate and comprehensible classification rules from ANN’s that have been trained using the discretization of the continuous attributes. The data pre-processing provides very good improvement of the ANN accuracy and consequently leads to improve the performance of the classification rules in terms of fidelity and coverage. The results show that our algorithm is very suitable for medical decision making, so an excellent average accuracy of 94.73 has been achieved for the Pima dataset and 99.36 for the breast cancer dataset.},
  keywords={type: neural_network, rule_extraction, multiobjective_genetic_algorithm, discretized_attributes},
  doi={10.1007/s11063-020-10357-x}
}


@Article{li2020,
AUTHOR = {Li, Jiawei and Li, Yiming and Xiang, Xingchun and Xia, Shu-Tao and Dong, Siyi and Cai, Yun},
TITLE = {TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1203},
URL = {https://www.mdpi.com/1099-4300/22/11/1203},
ISSN = {1099-4300},
ABSTRACT = {Deep Neural Networks (DNNs) usually work in an end-to-end manner. This makes the trained DNNs easy to use, but they remain an ambiguous decision process for every test case. Unfortunately, the interpretability of decisions is crucial in some scenarios, such as medical or financial data mining and decision-making. In this paper, we propose a Tree-Network-Tree (TNT) learning framework for explainable decision-making, where the knowledge is alternately transferred between the tree model and DNNs. Specifically, the proposed TNT learning framework exerts the advantages of different models at different stages: (1) a novel James&ndash;Stein Decision Tree (JSDT) is proposed to generate better knowledge representations for DNNs, especially when the input data are in low-frequency or low-quality; (2) the DNNs output high-performing prediction result from the knowledge embedding inputs and behave as a teacher model for the following tree model; and (3) a novel distillable Gradient Boosted Decision Tree (dGBDT) is proposed to learn interpretable trees from the soft labels and make a comparable prediction as DNNs do. Extensive experiments on various machine learning tasks demonstrated the effectiveness of the proposed method.},
DOI = {10.3390/e22111203},
keywords={type: deep_neural_networks, james–stein_decision_trees, distillable_gradient_boosted_decision_tree, interpretable_machine_learning, knowledge_distillation}
}

@ARTICLE{burkhardt2021,
AUTHOR={Burkhardt, Sophie and Brugger, Jannis and Wagner, Nicolas and Ahmadi, Zahra and Kersting, Kristian and Kramer, Stefan},   	 
TITLE={Rule Extraction From Binary Neural Networks With Convolutional Rules for Model Validation},      	
JOURNAL={Frontiers in Artificial Intelligence},      	
VOLUME={4},           	
YEAR={2021},      	  
URL={https://www.frontiersin.org/articles/10.3389/frai.2021.642263},       	
DOI={10.3389/frai.2021.642263},      	
ISSN={2624-8212},   
ABSTRACT={Classification approaches that allow to extract logical rules such as decision trees are often considered to be more interpretable than neural networks. Also, logical rules are comparatively easy to verify with any possible input. This is an important part in systems that aim to ensure correct operation of a given model. However, for high-dimensional input data such as images, the individual symbols, i.e. pixels, are not easily interpretable. Therefore, rule-based approaches are not typically used for this kind of high-dimensional data. We introduce the concept of first-order convolutional rules, which are logical rules that can be extracted using a convolutional neural network (CNN), and whose complexity depends on the size of the convolutional filter and not on the dimensionality of the input. Our approach is based on rule extraction from binary neural networks with stochastic local search. We show how to extract rules that are not necessarily short, but characteristic of the input, and easy to visualize. Our experiments show that the proposed approach is able to model the functionality of the neural network while at the same time producing interpretable logical rules. Thus, we demonstrate the potential of rule-based approaches for images which allows to combine advantages of neural networks and rule learning.},
keywords={type: k-term_DNF, stochastic_local_search, convolutional_neural_networks, logical_rules, rule_extraction, interpretability}
}

@article {shams2021,
	author = {Shams, Zohreh and Dimanov, Botty and Kola, Sumaiyah and Simidjievski, Nikola and Terre, Helena Andres and Scherer, Paul and Matja{\v s}ec, Ur{\v s}ka and Abraham, Jean and Jamnik, Mateja and Li{\`o}, Pietro},
	title = {REM: An Integrative Rule Extraction Methodology for Explainable Data Analysis in Healthcare},
	year = {2021},
	doi = {10.1101/2021.01.25.21250459},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Deep learning models are receiving increasing attention in clinical decision-making, however the lack of explainability impedes their deployment in day-to-day clinical practice. We propose REM, an explainable methodology for extracting rules from deep neural networks and combining them with rules from non-deep learning models. This allows integrating machine learning and reasoning for investigating basic and applied biological research questions. We evaluate the utility of REM in two case studies for the predictive tasks of classifying histological and immunohistochemical breast cancer subtypes from genotype and phenotype data. We demonstrate that REM efficiently extracts accurate, comprehensible rulesets from deep neural networks that can be readily integrated with rulesets obtained from tree-based approaches. REM provides explanation facilities for predictions and enables the clinicians to validate and calibrate the extracted rulesets with their domain knowledge. With these functionalities, REM caters for a novel and direct human-in-the-loop approach in clinical decision-making.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis work was supported by The Mark Foundation Institute for Integrated Cancer Medicine (MFICM). MFICM is hosted at the University of Cambridge, with funding from The Mark Foundation for Cancer Research (NY, U. S. A.) and the Cancer Research UK Cambridge Centre [C9685/A25177] (UK).Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:N/AAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesThe data used is publicly available.MLMachine LearningMRMachine ReasoningDNNDeep Neural NetworkREMRule Extraction MethodologyREM-DRule Extraction Methodology from Deep Neural NetworksREM-TRule Extraction Methodology from Trees},
	URL = {https://www.medrxiv.org/content/early/2021/04/28/2021.01.25.21250459},
	eprint = {https://www.medrxiv.org/content/early/2021/04/28/2021.01.25.21250459.full.pdf},
	journal = {medRxiv},
        %keywords={}
}


@misc{zarlenga2021,
  title={Efficient Decompositional Rule Extraction for Deep Neural Networks},
  author={Zarlenga, Mateo Espinosa and Shams, Zohreh and Jamnik, Mateja},
  journal={NeurIPS 2021 Workshop on eXplainable AI approaches for debugging and diagnosis (XAI4Debugging)},
  url={https://arxiv.org/abs/2111.12628},
  year={2021},  
  abstract={In recent years, there has been signiﬁcant work on increasing both interpretability and debuggability of a Deep Neural Network (DNN) by extracting a rule-based model that approximates its decision boundary. Nevertheless, current DNN rule extraction methods that consider a DNN’s latent space when extracting rules, known as decompositional algorithms, are either restricted to single-layer DNNs or intractable as the size of the DNN or data grows. In this paper, we address these limitations by introducing ECLAIRE, a novel polynomial-time rule extraction algorithm capable of scaling to both large DNN architectures and large training datasets. We evaluate ECLAIRE on a wide variety of tasks, ranging from breast cancer prognosis to particle detection, and show that it consistently extracts more accurate and comprehensible rule sets than the current state-of-the-art methods while using orders of magnitude less computational resources. We make all of our methods available, including a rule set visualisation interface, through the open-source REMIX library (https://github.com/mateoespinosa/remix).},
  %keywords={}
}

@article{lee2021,
author = {Lee, Hurnjoo and Kim, Hyeoncheol},
title = {Uncertainty of Rules Extracted from Artificial Neural Networks},
journal = {Applied Artificial Intelligence},
volume = {35},
number = {8},
pages = {589-604},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/08839514.2021.1922845},
URL = {https://doi.org/10.1080/08839514.2021.1922845  
},
eprint = {https://doi.org/10.1080/08839514.2021.1922845},
abstract={Artificial neural networks evolve into deep learning recently and perform well in various fields, such as image and speech recognition and translation. However, there is a problem that it is difficult for a person to understand what exactly the trained knowledge of an artificial neural network. As one of the methods for solving the problem of the artificial neural network, rule extraction methods had been devised. In this study, rules are extracted from artificial neural networks using ordered-attribute search (OAS) algorithm, which is one of the methods of extracting rules from trained neural networks, and the rules are analyzed to improve the extracted rules. As a result, we found that when the output value of the hidden layer has an intermediate value that is not close to 0 or 1 after passing through the sigmoid function, the problem of rule uncertainty occurs and affects the accuracy of the rules. In order to solve the uncertainty problem of the rules, we applied the hidden unit clarification method and suggested that it is possible to extract the efficient rule by binarizing the hidden layer output value. In addition, we extracted CDRPs (critical data routing paths) from the trained neural networks and used CDRPs to prune the extracted rules, which showed that the uncertainty problem of rules can be improved.},
%keywords={}

}
